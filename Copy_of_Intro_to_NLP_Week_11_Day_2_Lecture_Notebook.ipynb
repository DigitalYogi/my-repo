{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Intro to NLP Week 11 - Day 2 - Lecture Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DigitalYogi/my-repo/blob/main/Copy_of_Intro_to_NLP_Week_11_Day_2_Lecture_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnI4qsOubeiT"
      },
      "source": [
        "# Introduction to Deep NLP models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis. The tutorial is heavily based on [Tensorflow text classification](https://www.tensorflow.org/text/tutorials/text_classification_rnn) tutorial available online. We will explain the notebook in depth and discuss code as we go along."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t78yU0oeSl6"
      },
      "source": [
        "We will import the required packages here. Note that we are importing `tfds` here. Tensorflow datasets package has a dataset called IMDB Reviews. We will use this dataset to train a deep learning model for text classification. Here, given a review of a movie, we will predict whether the movie review is positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z682XYsrjkY9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# We use Tensorflow Datasets package to load the data ****\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXHa-w9JZhb"
      },
      "source": [
        "Import `matplotlib` and create a helper function to plot graphs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "# Step 1: Load and Preprocess the Dataset\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) for details on how to load this sort of data manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHRwRoP2nVHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6abe334-8f8d-48a2-d859-8c717c51b0fd"
      },
      "source": [
        "# Define the dataset\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "# Get the train and test split from the dataset\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "# Print some details of the train_dataset\n",
        "train_dataset.element_spec"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteEAA35K/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteEAA35K/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteEAA35K/imdb_reviews-unsupervised.tfrecord\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4_BGKyurao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b922f81-c239-4fe4-cd4d-f0d3f73e6b53"
      },
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64 # hyper parameter"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "# .shuffle(BUFFER_SIZE) --*** we want to randomize the sequence or the order in which the model sees the input data.\n",
        "# .batch(BATCH_SIZE) # 64 -> amount of data that is sent at once for optimization.\n",
        "# .prefetch(tf.data.AUTOTUNE) --***\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Ru3xMl1_nHC0",
        "outputId": "7f8cbafb-02fe-471f-c653-70e506adee61"
      },
      "source": [
        "# this will give an error because we cannot index prefetch dataset\n",
        "\n",
        "train_dataset[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7597fae7feb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this will give an error because we cannot index prefetch dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'PrefetchDataset' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIa4hPPKpBom",
        "outputId": "63c171f9-6562-46b3-cfe3-0bed5b3f59d0"
      },
      "source": [
        "train_dataset.take(1)\n",
        "\n",
        "# we see that this data has two types of variables in it"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((None,), (None,)), types: (tf.string, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJC3eL3omjwe"
      },
      "source": [
        "# this is the raw text dataset. now we need to preprocess the data."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqkvdcFv41wC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a0a5ac3-ecfd-4d83-89d1-e044649da9f0"
      },
      "source": [
        "# we have to iterate through the train_dataset to print the values.\n",
        "examples_to_print = 6\n",
        "\n",
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:examples_to_print])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:examples_to_print])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b\"Dil was a memorable movie that bring to the celluloid a great director like Indra Kumar. The movie followed with Beta, Ishq, Raja & Masti all of whom were superb.<br /><br />But then every successful director gives a few horrible movies alongwith some hits too. Pyare Mohan is one such movie.<br /><br />Though the comedies are told nicely but then they fail the viewer to laugh. Comparing with the kind of comedy movies being made today this is a dumb.<br /><br />If you really want to watch a movie and laugh, please don't watch this. Because the pathetic comedy will make you cry only.<br /><br />In short, the movie is worth a miss.\"\n",
            " b'There are lots of extremely good-looking people in this movie. That\\'s probably the best thing about it. Perhaps that even makes it worth watching.<br /><br />\"Loaded\" tells the story of Tristan Price (Jesse Metcalfe), a young man who\\'s about to make his mark on the world. He\\'s the son of a well-to-do family with a good reputation, and he\\'s on his way to law school. But like so many such settings, things aren\\'t quite as perfect as they appear. The expectations in this family far outweigh the love. Except for school, Tristan\\'s father rarely lets him leave the house. This seems to be the result of some past traumatic event that shook the family, which is partially revealed through flashbacks but isn\\'t spelled out until the very end. Tristan\\'s claustrophobic environment causes him to let loose in very extreme ways at the first possible opportunity, when his friends take him out to a strip club to celebrate his graduation. The celebration soon follows some strippers back to a beach house party, and from there, Tristan befriends Sebastian Cole (Corey Large), who pulls him into a drug dealing underworld.<br /><br />While technically well-made, this movie suffers from a lackluster script and a storyline that isn\\'t very engaging. Also counting against this film are some constant camera tricks that generally seemed annoying and out-of-place, such as slow-motion, fast-motion, freeze-frames and echos. These are the types of effects a director might normally utilize to show a character\\'s perspective while on drugs, except in this case they seem to have been sporadically tossed in at random points, in some cheap attempt at style.<br /><br />Despite its cast of relative unknowns, performances were good all around, most notably with respect to the main antagonist (Corey Large). I suspect we\\'ll be seeing at least a couple of these people in bigger and better projects in the future.<br /><br />Of course, when mentioning the actors, I must mention their looks. Rating based on hotness, this movies scores an 11. The women in this movie are incredible-looking and almost distract you from what a boring movie you\\'re watching. I\\'m sure the male characters are also quite attractive, but you\\'ll have to ask someone else to comment on that.<br /><br />Overall, I can\\'t recommend this movie, not for buying, renting, or even seeing for free. It\\'s unfortunately just not worth the effort it takes to sit through.'\n",
            " b'A fey story of a Martian attempt to colonize Earth. (Things must be pretty bad back on Mars.) Two state troopers investigate the scene of a reported UFO crash. Whatever landed is buried under the ice at Tracy\\'s Pond but there are footsteps in the snow leading to a nearby diner.<br /><br />The diner has had no customers since eleven o\\'clock that morning. Now there are a handful of bus passengers sitting around waiting for permission to cross a structurally weak bridge. The bus driver insists that six passengers were aboard the bus, although he didn\\'t notice who they were. The problem is that there are now SEVEN people waiting for the journey to be resumed. One of them is an alien, but which one? All of them are suspect. There\\'s the crazy old man (Jack Elam), of course, who seems to exercise a sub rosa wit. There\\'s a blustering businessman who must get to Boston (John Hoyt). A young couple on their honeymoon. (Execrable performance by the husband, Ron Kipling.) Except for the couples, nobody has noticed anyone else. And even the couples are suspicious of each other. Bride to newly minted husband: \"I could have sworn you had a mole on your chin.\" The story continues in a sprightly but slightly spooky way -- the phone rings for no reason, the lights go on and off, the juke box turns itself on -- and none of it is to be taken seriously.<br /><br />It\\'s a thoroughly enjoyable ensemble play and the climactic revelation is worth a chuckle. There is no discernible \"depth\" to it. It\\'s not a moral message about pod people masquerading as normal citizens. It\\'s not a warning of any kind, just a fairy tale that diverts and amuses.<br /><br />I always enjoy it when it\\'s on. It\\'s especially interesting to see John Hoyt as the irritable and impatient businessman, knowing that in 1954 he was the Roman Senator who masterminded the assassination of Julius Caesar in MGM\\'s version of Shakespeare\\'s play. And here he is -- with three arms.<br /><br />Oops.'\n",
            " b'Is there any other time period that has been so exhaustively covered by television (or the media in general) as the 1960s? No. And do we really need yet another trip through that turbulent time? Not really. But if we must have one, does it have to be as shallow as \"The \\'60s\"? <br /><br />I like to think that co-writers Bill Couturie and Robert Greenfield had more in mind for this two-part miniseries than what ultimately resulted, especially given Couturie\\'s involvement in the superb HBO movie \"Dear America: Letters Home From Vietnam\" which utilized little original music and no original footage, letting the sights and sounds of the time speak for themselves. This presentation intercuts file footage with the dramatic production, but it doesn\\'t do anyone any favours by trying to do too much in too little time; like so many of its ilk, it\\'s seen from the point of view of one family. But the children of the family seem to be involved tangentially with almost every major event of the \\'60s (it\\'s amazing that one of them doesn\\'t go to the Rolling Stones gig at Altamont), making it seem less like a period drama and more like a Cliff Notes version of the decade.<br /><br />The makers rush through it so much that there\\'s little or no time to give the characters any character, with the stick figures called our protagonists off screen for ages at a time - the children\\'s father is especially clich\\xc3\\xa9d - and then when they\\'re back on BLAMMO! it\\'s something else. Garry Trudeau could teach the filmmakers a thing or two about doing this kind of thing properly. In fairness, Jerry O\\'Connell, Jordana Brewster, Jeremy Sisto, Julia Stiles and Charles S. Dutton give their material the old college try, but they\\'re wasted (especially the latter two); it\\'s undeniably good to see David Alan Grier in a rare straight role as activist Fred Hampton, and Rosanna Arquette (in an uncredited cameo in part 2) is always welcome.<br /><br />What isn\\'t welcome is how \"The \\'60s\" drowns the soundtrack with so many period songs that it ultimately reduces its already minimal effect (and this may well be the only time an American TV presentation about post-60s America never mentions the British Invasion - no Beatles, no Rolling Stones... then again, there\\'s only so much tunes you can shoehorn into a soundtrack album, right?). Capping its surface-skimming approach to both the time and the plot with an almost out-of-place happy ending, \"American Dreams\" and \"The Wonder Years\" did it all much, much better. Nothing to see here you can\\'t see elsewhere, people... except for Julia Stiles doing the twist, that is.'\n",
            " b'I have never seen such terrible performances in all my life.<br /><br />Everyone in the entire film was absolute rubbish.<br /><br />Not one decent actor/actress in the whole film, it was a joke.<br /><br />Reminded me of drama at school...'\n",
            " b\"It's amazing that from a good, though not wonderful, film made back in the early Nineties, a whole franchise can grow. 'Stargate; SG1' is, without a doubt, a worthy addition to the science fiction genre and has the right to stand shoulder-to-shoulder with 'Star Trek' as the kings of sci-fi.<br /><br />Following on from the 1994 feature film 'Stargate', this series sees Stargate command (a military/science organisation) figuring out that the stargate system can be used to travel to various planets across the galaxy and beyond and the military sets up a number of teams to explore. SG1 is one such team, headed by military veteran Colonel Jack O'Neill, and includes archaeologist Doctor Daniel Jackson, military scientist Captain Samantha Carter and alien Teal'c, who has betrayed his overlord leaders in the hopes of one day freeing his people. Earth quickly makes an enemy of the Goa'uld, a parasitic race who use humans as hosts and think themselves equal to gods.<br /><br />The top-notch cast have much to be congratulated for in bringing this show to life. Richard Dean Anderson is perfect as the cynical and sarcastic O'Neill, who can shift from boyish to deadly in the blink of an eye. Michael Shanks, as Daniel, brings heart and an will of steel to the character, who has grown from wide-eyed innocence to darker and more hard-bitten as the show has progressed. Amanda Tapping, as Carter, has perfected the balance between depicting her character's femininity without comprising the fact she is a strong, intelligent military scientist. Christopher Judge is excellent as the aloof Teal'c, who is able to depict the character's emotions with subtlety. And Don S Davis is perfect as the esteemed General Hammond who leads with a good balance of fairness and firmness.<br /><br />Almost all the episodes are are involving and portrayed with intelligence, reflecting on moral dilemmas as well as the friction between military interests and civilian beliefs (often shown through arguments between O'Neill and Jackson). Guest characters are solidly depicted and story arcs are handled in a manner that doesn't bore viewers. SG1 also excels in humour, from O'Neill's wisecracks to episodes that are just wacky and odd! SG1 has everything from action to drama to romance to suspense to the heartbreaking scenes of death. It isn't just an excellent sci-fi show but is an excellent show, overall.\"]\n",
            "\n",
            "labels:  [0 0 1 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmHDgjffmp39"
      },
      "source": [
        "## Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "### Create the text encoder in Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "From [Tensorflow TextVectorization documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization):\n",
        "\n",
        "> This layer has basic options for managing text in a Keras model. It transforms a batch of strings (one example = one string) into either a list of token indices (one example = 1D tensor of integer token indices) or a dense representation (one example = 1D tensor of float values representing data about the example's tokens).\n",
        "\n",
        "> If desired, the user can call this layer's adapt() method on a dataset. When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a 'vocabulary' from them. This vocabulary can have unlimited size or be capped, depending on the configuration options for this layer; if there are more unique values in the input than the maximum vocabulary size, the most frequent terms will be used to create the vocabulary.\n",
        "\n",
        "> The processing of each example contains the following steps:\n",
        "1. Standardize each example (usually lowercasing + punctuation stripping)\n",
        "2. Split each example into substrings (usually words)\n",
        "3. Recombine substrings into tokens (usually ngrams)\n",
        "4. Index tokens (associate a unique int value with each token)\n",
        "5. Transform each example using this index, either into a vector of ints or a dense float vector.\n",
        "\n",
        "Now, let us create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Temoa5tbk216"
      },
      "source": [
        "```python\n",
        "tf.keras.layers.TextVectorization(\n",
        "    max_tokens=None, standardize='lower_and_strip_punctuation',\n",
        "    split='whitespace', ngrams=None, output_mode='int',\n",
        "    output_sequence_length=None, pad_to_max_tokens=False, vocabulary=None, **kwargs\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "source": [
        "# The maximum number of words that the vocabulary can have\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE, # set the max number of words to be used in the Bag of Words.\n",
        "    standardize='lower_and_strip_punctuation', # standardization - lowercase the sentence and strip all puctuations.\n",
        "    split='whitespace', # tokenization - split the sentence based on whitespace.\n",
        "    ngrams=None, # Do not create any n-grams.\n",
        "    output_mode='int') # We need our output as a list of integers.\n",
        "\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text)) # adapt the train_dataset and run the text processing pipeline on it."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl-_U0EnmBn7"
      },
      "source": [
        "```python\n",
        "\n",
        "# Python code to reproduce tensorflow TextVectorization\n",
        "\n",
        "samples_tr = train_dataset.take(1)\n",
        "[(example, label) for example, label in samples_tr]\n",
        "first_sentence = example[0].numpy()\n",
        "first_sentence\n",
        "\n",
        "# Python code to reproduce tensorflow TextVectorization\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "reivew = str(first_sentence)\n",
        "# lower case the corpus\n",
        "reivew = reivew.lower()\n",
        "\n",
        "# removing punctuations\n",
        "# !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
        "reivew = reivew.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# tokenize sentences to words\n",
        "tokenized_reivew = word_tokenize(reivew)\n",
        "print(\"Tokenized reivew: \", tokenized_reivew)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQzVBbe3Ldu"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Short sentences are padded with a padding token in such a way that the length of the sentences are the same. Padding tokens doesn't affect the learning ability of the network as they are not considered towards the gradient updates. Any words in a new sentence that is not already in the vocabulary will be marked with an unknown token. After the padding and unknown tokens they're sorted by frequency: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBoyjjWg0Ac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf1e019-ebf1-453d-dff7-3b0dad74c8b5"
      },
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]\n",
        "\n",
        "# '[UNK]' -> unknown token - to specify any words that are unknown to TensorFlow."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGc7C9WiwRWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115ba298-8ce7-4df2-f2ce-eeb7b4732d14"
      },
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1,  14,   4, ...,   0,   0,   0],\n",
              "       [ 48,  24, 741, ...,   0,   0,   0],\n",
              "       [  4,   1,  64, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVslGKEWo_Ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9ddd2b-b706-4d2f-b869-b147ca8d1f77"
      },
      "source": [
        "encoded_example.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 918)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a70a-bKB88l8"
      },
      "source": [
        "# Step 2: Visualize the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
        "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYzaehj3YF59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20155d3-8c46-47a7-ca66-ce5f8f4b3abb"
      },
      "source": [
        "num_words = 15\n",
        "n = 0\n",
        "\n",
        "words_in_the_sentence = str(example[n].numpy()).split(' ')[:num_words] # slicing 15 words out of the first sentence in the dataset\n",
        "encodeded_id_of_the_words = encoded_example[n][:num_words] # slicing 15 words out of the encoded output of first sentence in the dataset\n",
        "\n",
        "print(\"Encoding\\tWord\")\n",
        "for word, encoded_id in zip(words_in_the_sentence, encodeded_id_of_the_words):\n",
        "  print(encoded_id, \"\\t\\t\", word)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding\tWord\n",
            "1 \t\t b\"Dil\n",
            "14 \t\t was\n",
            "4 \t\t a\n",
            "892 \t\t memorable\n",
            "18 \t\t movie\n",
            "12 \t\t that\n",
            "693 \t\t bring\n",
            "6 \t\t to\n",
            "2 \t\t the\n",
            "1 \t\t celluloid\n",
            "4 \t\t a\n",
            "85 \t\t great\n",
            "172 \t\t director\n",
            "39 \t\t like\n",
            "1 \t\t Indra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNclBwnfXO_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf62f13c-75ea-4f83-d7fe-2e906d7d7a1a"
      },
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b\"Dil was a memorable movie that bring to the celluloid a great director like Indra Kumar. The movie followed with Beta, Ishq, Raja & Masti all of whom were superb.<br /><br />But then every successful director gives a few horrible movies alongwith some hits too. Pyare Mohan is one such movie.<br /><br />Though the comedies are told nicely but then they fail the viewer to laugh. Comparing with the kind of comedy movies being made today this is a dumb.<br /><br />If you really want to watch a movie and laugh, please don't watch this. Because the pathetic comedy will make you cry only.<br /><br />In short, the movie is worth a miss.\"\n",
            "Round-trip:  [UNK] was a memorable movie that bring to the [UNK] a great director like [UNK] [UNK] the movie [UNK] with [UNK] [UNK] [UNK] [UNK] all of whom were [UNK] br but then every [UNK] director gives a few horrible movies [UNK] some [UNK] too [UNK] [UNK] is one such moviebr br though the [UNK] are told [UNK] but then they [UNK] the viewer to laugh [UNK] with the kind of comedy movies being made today this is a [UNK] br if you really want to watch a movie and laugh please dont watch this because the [UNK] comedy will make you [UNK] [UNK] br in short the movie is worth a miss                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            "\n",
            "Original:  b'There are lots of extremely good-looking people in this movie. That\\'s probably the best thing about it. Perhaps that even makes it worth watching.<br /><br />\"Loaded\" tells the story of Tristan Price (Jesse Metcalfe), a young man who\\'s about to make his mark on the world. He\\'s the son of a well-to-do family with a good reputation, and he\\'s on his way to law school. But like so many such settings, things aren\\'t quite as perfect as they appear. The expectations in this family far outweigh the love. Except for school, Tristan\\'s father rarely lets him leave the house. This seems to be the result of some past traumatic event that shook the family, which is partially revealed through flashbacks but isn\\'t spelled out until the very end. Tristan\\'s claustrophobic environment causes him to let loose in very extreme ways at the first possible opportunity, when his friends take him out to a strip club to celebrate his graduation. The celebration soon follows some strippers back to a beach house party, and from there, Tristan befriends Sebastian Cole (Corey Large), who pulls him into a drug dealing underworld.<br /><br />While technically well-made, this movie suffers from a lackluster script and a storyline that isn\\'t very engaging. Also counting against this film are some constant camera tricks that generally seemed annoying and out-of-place, such as slow-motion, fast-motion, freeze-frames and echos. These are the types of effects a director might normally utilize to show a character\\'s perspective while on drugs, except in this case they seem to have been sporadically tossed in at random points, in some cheap attempt at style.<br /><br />Despite its cast of relative unknowns, performances were good all around, most notably with respect to the main antagonist (Corey Large). I suspect we\\'ll be seeing at least a couple of these people in bigger and better projects in the future.<br /><br />Of course, when mentioning the actors, I must mention their looks. Rating based on hotness, this movies scores an 11. The women in this movie are incredible-looking and almost distract you from what a boring movie you\\'re watching. I\\'m sure the male characters are also quite attractive, but you\\'ll have to ask someone else to comment on that.<br /><br />Overall, I can\\'t recommend this movie, not for buying, renting, or even seeing for free. It\\'s unfortunately just not worth the effort it takes to sit through.'\n",
            "Round-trip:  there are lots of extremely [UNK] people in this movie thats probably the best thing about it perhaps that even makes it worth [UNK] br [UNK] tells the story of [UNK] [UNK] [UNK] [UNK] a young man whos about to make his mark on the world hes the son of a [UNK] family with a good [UNK] and hes on his way to [UNK] school but like so many such [UNK] things arent quite as perfect as they appear the [UNK] in this family far [UNK] the love except for school [UNK] father [UNK] lets him leave the house this seems to be the result of some past [UNK] [UNK] that [UNK] the family which is [UNK] [UNK] through [UNK] but isnt [UNK] out until the very end [UNK] [UNK] [UNK] [UNK] him to let [UNK] in very [UNK] ways at the first possible [UNK] when his friends take him out to a [UNK] [UNK] to [UNK] his [UNK] the [UNK] soon [UNK] some [UNK] back to a [UNK] house [UNK] and from there [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] who [UNK] him into a [UNK] [UNK] [UNK] br while [UNK] [UNK] this movie [UNK] from a [UNK] script and a storyline that isnt very [UNK] also [UNK] against this film are some [UNK] camera [UNK] that [UNK] seemed annoying and [UNK] such as [UNK] [UNK] [UNK] and [UNK] these are the [UNK] of effects a director might [UNK] [UNK] to show a characters [UNK] while on [UNK] except in this case they seem to have been [UNK] [UNK] in at [UNK] points in some cheap attempt at [UNK] br despite its cast of [UNK] [UNK] performances were good all around most [UNK] with [UNK] to the main [UNK] [UNK] [UNK] i [UNK] well be seeing at least a couple of these people in [UNK] and better [UNK] in the [UNK] br of course when [UNK] the actors i must mention their looks rating based on [UNK] this movies [UNK] an [UNK] the women in this movie are [UNK] and almost [UNK] you from what a boring movie youre watching im sure the male characters are also quite [UNK] but youll have to ask someone else to comment on [UNK] br overall i cant recommend this movie not for [UNK] [UNK] or even seeing for free its unfortunately just not worth the effort it takes to sit through                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
            "\n",
            "Original:  b'A fey story of a Martian attempt to colonize Earth. (Things must be pretty bad back on Mars.) Two state troopers investigate the scene of a reported UFO crash. Whatever landed is buried under the ice at Tracy\\'s Pond but there are footsteps in the snow leading to a nearby diner.<br /><br />The diner has had no customers since eleven o\\'clock that morning. Now there are a handful of bus passengers sitting around waiting for permission to cross a structurally weak bridge. The bus driver insists that six passengers were aboard the bus, although he didn\\'t notice who they were. The problem is that there are now SEVEN people waiting for the journey to be resumed. One of them is an alien, but which one? All of them are suspect. There\\'s the crazy old man (Jack Elam), of course, who seems to exercise a sub rosa wit. There\\'s a blustering businessman who must get to Boston (John Hoyt). A young couple on their honeymoon. (Execrable performance by the husband, Ron Kipling.) Except for the couples, nobody has noticed anyone else. And even the couples are suspicious of each other. Bride to newly minted husband: \"I could have sworn you had a mole on your chin.\" The story continues in a sprightly but slightly spooky way -- the phone rings for no reason, the lights go on and off, the juke box turns itself on -- and none of it is to be taken seriously.<br /><br />It\\'s a thoroughly enjoyable ensemble play and the climactic revelation is worth a chuckle. There is no discernible \"depth\" to it. It\\'s not a moral message about pod people masquerading as normal citizens. It\\'s not a warning of any kind, just a fairy tale that diverts and amuses.<br /><br />I always enjoy it when it\\'s on. It\\'s especially interesting to see John Hoyt as the irritable and impatient businessman, knowing that in 1954 he was the Roman Senator who masterminded the assassination of Julius Caesar in MGM\\'s version of Shakespeare\\'s play. And here he is -- with three arms.<br /><br />Oops.'\n",
            "Round-trip:  a [UNK] story of a [UNK] attempt to [UNK] earth things must be pretty bad back on [UNK] two [UNK] [UNK] [UNK] the scene of a [UNK] [UNK] [UNK] whatever [UNK] is [UNK] under the [UNK] at [UNK] [UNK] but there are [UNK] in the [UNK] leading to a [UNK] [UNK] br the [UNK] has had no [UNK] since [UNK] [UNK] that [UNK] now there are a [UNK] of [UNK] [UNK] [UNK] around [UNK] for [UNK] to [UNK] a [UNK] weak [UNK] the [UNK] [UNK] [UNK] that [UNK] [UNK] were [UNK] the [UNK] although he didnt [UNK] who they were the problem is that there are now [UNK] people [UNK] for the [UNK] to be [UNK] one of them is an [UNK] but which one all of them are [UNK] theres the crazy old man jack [UNK] of course who seems to [UNK] a [UNK] [UNK] [UNK] theres a [UNK] [UNK] who must get to [UNK] john [UNK] a young couple on their [UNK] [UNK] performance by the husband [UNK] [UNK] except for the [UNK] [UNK] has [UNK] anyone else and even the [UNK] are [UNK] of each other [UNK] to [UNK] [UNK] husband i could have [UNK] you had a [UNK] on your [UNK] the story [UNK] in a [UNK] but [UNK] [UNK] way the [UNK] [UNK] for no reason the [UNK] go on and off the [UNK] [UNK] turns itself on and none of it is to be taken [UNK] br its a [UNK] enjoyable [UNK] play and the [UNK] [UNK] is worth a [UNK] there is no [UNK] [UNK] to it its not a [UNK] message about [UNK] people [UNK] as [UNK] [UNK] its not a [UNK] of any kind just a [UNK] tale that [UNK] and [UNK] br i always enjoy it when its on its especially interesting to see john [UNK] as the [UNK] and [UNK] [UNK] [UNK] that in [UNK] he was the [UNK] [UNK] who [UNK] the [UNK] of [UNK] [UNK] in [UNK] version of [UNK] play and here he is with three [UNK] br [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "# Step 3: Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zsmInBOCPO"
      },
      "source": [
        "![A drawing of the information flow in the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model. \n",
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fodCI7soQi"
      },
      "source": [
        "The code to implement this is below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87a8-CwfKebw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626e3fce-d87e-4232-e4b2-8f070341f9d4"
      },
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlS0iaUIWLpI"
      },
      "source": [
        "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41gw3KfWHus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba194b1-c6bd-4b48-d664-47546ad9408b"
      },
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00152886]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VQmGnEWcuz"
      },
      "source": [
        "Now, evaluate it again in a batch with a longer sentence. The result should be identical:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIgpuTeFNDzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59fd4d1-50cf-42b4-8855-e30629b2f02e"
      },
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00152886]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj2xei41YZjC"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "# Step 4: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw86wWS4YgR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16160405-d785-4b40-ba5e-2d769f6bca62"
      },
      "source": [
        "history = model.fit(train_dataset, epochs=5)\n",
        "\n",
        "# changed previous line to enable validation split of th\n",
        "#history = model.fit(X[train_dataset], dummy_y[train_dataset], validation_split=0.33,nb_epoch=200, batch_size=5, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 36/391 [=>............................] - ETA: 11:23 - loss: 0.3154 - acc: 0.8563"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52iUqFJb7cZa"
      },
      "source": [
        "# Step 5: Evaluate the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaNbXi43YgUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2061cff1-c29a-4233-8b24-56cacb6116bc"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 176s 443ms/step - loss: 0.3462 - accuracy: 0.8574\n",
            "Test Loss: 0.34621840715408325\n",
            "Test Accuracy: 0.8573600053787231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOBIlS6K11HM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503ed80e-59d3-42ea-ca43-2fae4525e51e"
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'accuracy'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZmwt_mzaQJk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "outputId": "ae18d5c5-18fc-48d0-e394-2ffb66066eb8"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-bbddbdf6a590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6b772c694a7e>\u001b[0m in \u001b[0;36mplot_graphs\u001b[0;34m(history, metric)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_accuracy'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAHSCAYAAACU+XDdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRc533e8eeHnVhIggAIUdwXgJRky1pobXRIMLZsObGtnGM3R3KT2K0bJW2cZmnTIzeJEyv1sU/S1m0cNbaTKnHjJKrr9qSUTVdRooFoU5YsyJZkk8KA4CIuIjmDjdiIbebXP2ZIDUGAGBAD3Jm53885OJy5cwd8MCTn4bz3vvc1dxcAAGFUEnQAAACCQgkCAEKLEgQAhBYlCAAILUoQABBalCAAILTKgg4wXWNjo2/atCnoGACAIvHyyy/3uHvTTI/lXQlu2rRJHR0dQccAABQJM3tjtscYDgUAhBYlCAAILUoQABBalCAAILQoQQBAaFGCAIDQogQBAKFFCQIAQosSBACEFiUIAAgtShAAEFqUIAAgtChBAEBoUYIAgNCiBAEAoUUJAgBCixIEAOQVd9foxJSGx6cW/ffKu5XlAQDFZWwyob6RCfWNTGhgdFJ9oxPqT9/vH83YnnF/fCqpf75rsz79wZsXNRslCADI2thk4qrC6h+dUP/I5BX3M4vt4mRi1u+3Ylm5VtVUqL66XDeurNLNNy5P36/QbetXLvrPQwkCQEiNT11ZaP0jV35KGxidUN/o5BWf2kYnZi+05VVlWlVToZXVFWpeXqUdNyzXqppy1adLrb66QqtqKlLbqiu0Ylm5ykqDPSpHCQJAEZhMJN8qsis+jU2ob9qntEv7XeuYW11lWaq8airUWFuhltW1qq+puPwp7VKRXSq4ldXlKg+40K4HJQgAeWYykdTA6GS6rC4V2rQiG3nrU1r/yISGrlFotZVlqr9UWtUV2tpUe7nIVlZnFluF6mvKtXJZhSrKCq/QrgclCACLKJF0DYy+VWRvDTNeGnaczPh0liq5wbHZC626ojSjsCq0qaH6ivurqisuF15qaLJclWWlS/gTFxZKEACylEi6LlycvKKwLpXbwLSTQvrTx9oGxyblPvP3W1ZeermoVtVUaH199eVPZZlFllloVeUUWi5RggAwzYXRSXXFhhQ9N6Su86lfj8aH1TsyMWuhVZaVXFFYa+urVV9dftWntEuFV19doWUVFFrQKEEAoTU6MaXu2PBbZXd+WF3nhnRucOzyPnWVZWpprtW7dzSreUWV6jNK7FK51VeXa1l5qcwswJ8G14MSBFD0JqaSOt4zouj5IXWdG0r9en5IJ/tGL3+yqywrUUtzre7b1qDtzXVqvaFO25vrtGZFFeVWxChBAEUjkXSd6hu9quyOxUc0lUy1XWmJaUtjjd62doU+fMc6tTbXafsNddqwqlqlJZRd2FCCAAqOu+vshTF1nb90zG5YXeeHdCQ2pLHJ5OX9NqyqVmtzne6/ufly2W1urOFsSVxGCQLIa73D4xmf7FJl13Vu6Ip5cc3LK9XaXKefu3vj5WHMbatrVVPJWxyuLau/IWb2gKT/KqlU0p+7++enPb5B0lclrUzv86i77zezTZJelxRN7/qCu/9ybqIDKCZDY5PqSpfcpRNVus4PqWd44vI+K5aVa/sNdfqZ29deLrvW5lqtrK4IMDkK2ZwlaGalkh6XdL+k05JeMrN97n44Y7ffkfR1d/9TM7tZ0n5Jm9KPHXX323IbG0ChGptMqDuWLrv0p7qu88M6M3Dx8j7VFaVqaa7TT+5YfXkYc3tznZrqKjlJBTmVzSfBuyR1u/sxSTKzJyU9KCmzBF3S8vTtFZLezGVIAIVnKpHUid4RRc8NZ5TdkE70jih9jorKS01bm2q1c1O9Ptq8QdvThbd25TKVcJIKlkA2JbhW0qmM+6cl3T1tn9+X9Pdm9quSaiS9J+OxzWb2Q0mDkn7H3b8z/Tcws0ckPSJJGzZsyDo8gOAlk64zAxcVzTgbM3oudUbmRCJ1kkqJSZsaatTaXKcPvOPGdNnVamNDTUFedBnFI1dHjR+W9Jfu/p/M7F5Jf2Vmb5N0VtIGd+81szsl/Z2Z3eLug5lPdvevSPqKJO3cuXOW6zEACJK7Kz6UOkklc3L5kfNDVyyvs3blMrU212rP9qb0MbvUSSpc7gv5KJsSPCNpfcb9deltmT4h6QFJcvfvmVmVpEZ3j0kaT29/2cyOSmqV1LHQ4AAWz8DohLrOD181325gdPLyPo21FWptrtPP7lyv7Tekyq6luVbLq8oDTA7MTzYl+JKkFjPbrFT5PSTpo9P2OSnp3ZL+0sxuklQlKW5mTZL63D1hZlsktUg6lrP0ABZkdGJKR2You/OD45f3qassU+sNdXr/29Zoe3OtWtOF11hbGWByIDfmLEF3nzKzT0p6WqnpD0+4+yEze0xSh7vvk/RvJP2Zmf2GUifJfNzd3cx2S3rMzCYlJSX9srv3LdpPA2BGE1NJHevJuEZmenL5yb7Ry/tcumzYrm2NXDYMoWE+2yXRA7Jz507v6GC0FLgeiaTrZN9oxjG71Ce84z1XXzbsrXl2XDYMxc3MXnb3nTM9xuUUgAJ19sJFdZ67chjzyPlhjU9dfdmw997CZcOAmVCCQIE5Fh/WZ7/1uv6xM3Z526XLhv38PW9dNqyluVbVFfwTB66FfyFAgRgcm9QX//GI/vL5E6osK9Vv3t+qe7Y0cNkwYAEoQSDPJZKu/9VxSn/0dFR9oxP62TvX69++b7ua6jg7E1goShDIYy8e69Vnnjqsw2cH9c5N9frqB+/S29auCDoWUDQoQSAPne4f1ee+3alvvXZWN66o0hcfvl0fuHUNUxWAHKMEgTwyOjGlL7Uf1ZcPHJOZ9OvvadEv7d6qZRWczQksBkoQyAPurv/7ypv6/Lc7dW5wTB96x4169P07dOPKZUFHA4oaJQgE7NVTA/rMU4f0g5MDevvaFfriR2/XOzetCjoWEAqUIBCQ2OCY/vDpqL7x8mk11lbqDz9yqz5yxzrW0QOWECUILLGxyYSeOHhcjz/brcmE65f3bNWv7N2qOlZfAJYcJQgsEXfX3x8+r89+63Wd7BvV/Tc367d/6iZtaqwJOhoQWpQgsAQ6zw3qsacO6/mjvWptrtXXPnG33tXSGHQsIPQoQWAR9Y1M6AvPdOmvX3xDdVXleuzBW/TRuzaorLQk6GgARAkCi2IykdTXXnhDX3imSyMTCf38PRv16+9pVX0N1/gE8gklCOTYc11x/cE3D6s7NqyfaGnU737gZrU21wUdC8AMKEEgRzKXONrUUK0/+4Wdes9Nq7nUGZDHKEFggQbHJvUnz3brLw4eV2VZqT71/h36+K5NLFwLFABKELhOl5Y4+o9/H1XvyIT+yZ3r9G/ft12r66qCjgYgS5QgcB2+f7xPn3nqkA69OaidG+v1Fx+/S29fxxJHQKGhBIF5YIkjoLhQgkAWRiem9KXnjunLzx1liSOgiFCCwDW4u/a9+qY+t58ljoBiRAkCs3jt9IA+89RhvfxGv962djlLHAFFiBIEprlqiaMP36qP3MkSR0AxogSBtPGphJ747gn9ybNHNJFI6pf2bNEn925jiSOgiFGCCD2WOALCixJEqEXPDemxbx7Swe5etayu1V994i79REtT0LEALBFKEKHUPzKh/5yxxNFnPnSL/undLHEEhA0liFCZTCT11y+8oS/8wxENj0+xxBEQcpQgQuNAeomjI7FhvWtboz79QZY4AsKOEkTRO94zos9+67D+4fWYNrLEEYAMlCCKFkscAZgLJYiik0i6vvHyKf3R0yxxBODaKEEUFZY4AjAflCCKwpmBi/rc/tf1zdfOas2KKv3xw7frgyxxBGAOlCAKGkscAVgIShAF6dISR5//dqfOXhjTB9NLHK1liSMA80AJouBMX+Lojx9miSMA14cSRMGIDY3pj/5fVN/4wWk11LDEEYCFowSR98anEvqLgyf0xX9MLXH0yG6WOAKQG5Qg8pa765nD5/XZ/a/rjd5RveemZv32T9+kzSxxBCBHKEHkJZY4ArAUKEHklf6RCX3hH7r0tRdY4gjA4qMEkRdY4ghAEChBBO47R+J67Km3ljj63Q/crO03sMQRgMVHCSIwJ3pG9B++9br+4fXzLHEEIBCUIJbcUHqJoycOHldFaYkeff8O/TOWOAIQAEoQS4YljgDkG0oQS+KlE6kljn58ZlB3bqzXEx9/p25dtzLoWABCjhLEojozcFGf/3annnr1TZY4ApB3KEEsiosTCX3puaP68oGjkqRfe3eLfnkPSxwByC+UIHLK3fXUa2f1uf2vs8QRgLxHCSJnfnT6gj7z1CF1sMQRgAJBCSInvvbCG/rd//tjNdRU6A8/fKs+fOc6lbLEEYA8RwkiJ776/Andum6lvvaJu1jiCEDB4KrEWLBTfaM6EhvWB29dQwECKCiUIBasvSsuSdq7Y3XASQBgfrIqQTN7wMyiZtZtZo/O8PgGM4uY2Q/N7DUz+6mMxz6Vfl7UzN6Xy/DID+2dMW1YVa0tLHYLoMDMWYJmVirpcUnvl3SzpIfN7OZpu/2OpK+7++2SHpL039LPvTl9/xZJD0j6b+nvhyIxNpnQwaM92ru9iQnwAApONp8E75LU7e7H3H1C0pOSHpy2j0tanr69QtKb6dsPSnrS3cfd/bik7vT3Q5F48XifxiaTamMoFEAByqYE10o6lXH/dHpbpt+X9HNmdlrSfkm/Oo/nooBFOmOqLCvRvVsago4CAPOWqxNjHpb0l+6+TtJPSforM8v6e5vZI2bWYWYd8Xg8R5GwFNqjMd23tUFV5YxyAyg82RTVGUnrM+6vS2/L9AlJX5ckd/+epCpJjVk+V+7+FXff6e47m5qask+PQB3vGdGJ3lHOCgVQsLIpwZcktZjZZjOrUOpEl33T9jkp6d2SZGY3KVWC8fR+D5lZpZltltQi6fu5Co9gRTpjkqS2VkoQQGGa84ox7j5lZp+U9LSkUklPuPshM3tMUoe775P0byT9mZn9hlInyXzc3V3SITP7uqTDkqYk/Yq7Jxbrh8HSikRj2tpUow0N1UFHAYDrktVl09x9v1InvGRu+3TG7cOSds3y3M9K+uwCMiIPjU5M6cVjffqFezcGHQUArhtXjMF1eb67VxOJJMcDARQ0ShDXJRKNqaaiVDs31QcdBQCuGyWIeXN3tUfj2rWtUZVlTI0AULgoQczbkdiwzgxcZCgUQMGjBDFvl6dGbGdOJ4DCRgli3iLRmHbcUKc1K5YFHQUAFoQSxLwMjk2q40Q/Q6EAigIliHk5eKRHU0nX3u2UIIDCRwliXiLRmOqqynTHhpVBRwGABaMEkTV3VyQa1+7WJpWV8lcHQOHjnQxZO/TmoOJD42pr5axQAMWBEkTWnutKrfW4h6kRAIoEJYisRTpjevvaFVpdVxV0FADICUoQWRkYndAPTvZrL58CARQRShBZOXCkR0mX2pgfCKCIUILISntnTPXV5XrHOqZGACgelCDmlEy62rvi2tPapNISCzoOAOQMJYg5vXbmgvpGJrhUGoCiQwliTpHOmMyk3S2cFAOguFCCmFN7NKbb169UfU1F0FEAIKcoQVxTfGhcr56+wAWzARQlShDXdCB9lRiOBwIoRpQgrikSjamprlI3r1kedBQAyDlKELOaSiR1oCuuttYmlTA1AkARogQxqx+eGtDg2BRDoQCKFiWIWUU6YyotMb2rpTHoKACwKChBzCoSjWvnxnotryoPOgoALApKEDM6d2FMr58dZCgUQFGjBDGj9mhMkpgfCKCoUYKYUSQa040rqtTaXBt0FABYNJQgrjIxldR3j/SobcdqmTE1AkDxogRxlY4TfRqZSDAUCqDoUYK4SiQaU0Vpie7b2hB0FABYVJQgrhKJxnX3llWqqSwLOgoALCpKEFc41Teq7tiw2hgKBRAClCCu8NbUCBbQBVD8KEFcIRKNa2NDtTY31gQdBQAWHSWIy8YmE3r+aI/2bmdqBIBwoARx2QvHejU2mVQbQ6EAQoISxGXt0biqykt0zxamRgAIB0oQkiR317OdMd23tVFV5aVBxwGAJUEJQpJ0vGdEJ/tGOSsUQKhQgpCUOitUEvMDAYQKJQhJqfmB21bXav2q6qCjAMCSoQShkfEpvXisj6FQAKFDCULPH+3VRCLJqhEAQocShCLRmGoqSrVz06qgowDAkqIEQ87d1d4Z07taGlVRxl8HAOHCu17IdZ0f1psXxhgKBRBKlGDIRdKrRjA1AkAYUYIhF+mM6aY1y3XDiqqgowDAkqMEQ2xwbFIdb/QzNQJAaFGCIfbdIz1KJF17dzAUCiCcKMEQi3TGtLyqTLevXxl0FAAIBCUYUsmkq70rrt2tTSor5a8BgHDi3S+kDp8dVHxonKkRAEKNEgypSGdqasQeTooBEGKUYEhFojG9Y90KNdZWBh0FAAJDCYZQ38iEfnhqgAnyAEIvqxI0swfMLGpm3Wb26AyPf8HMXkl/dZnZQMZjiYzH9uUyPK7Pd47E5S6mRgAIvbK5djCzUkmPS7pf0mlJL5nZPnc/fGkfd/+NjP1/VdLtGd/iorvflrvIWKhIZ0wNNRW6de2KoKMAQKCy+SR4l6Rudz/m7hOSnpT04DX2f1jS3+YiHHIvkXQ91xXXntYmlZRY0HEAIFDZlOBaSacy7p9Ob7uKmW2UtFnSsxmbq8ysw8xeMLOfue6kyIlXTw+of3RSbQyFAsDcw6Hz9JCkb7h7ImPbRnc/Y2ZbJD1rZj9y96OZTzKzRyQ9IkkbNmzIcSRkau+MqcSk3S2NQUcBgMBl80nwjKT1GffXpbfN5CFNGwp19zPpX49JateVxwsv7fMVd9/p7jubmpi3tpgi0bju2FCvldUVQUcBgMBlU4IvSWoxs81mVqFU0V11lqeZ7ZBUL+l7GdvqzawyfbtR0i5Jh6c/F0sjPjSuH525wFmhAJA253Cou0+Z2SclPS2pVNIT7n7IzB6T1OHulwrxIUlPurtnPP0mSV82s6RShfv5zLNKsbSe64pLktq4SgwASMrymKC775e0f9q2T0+7//szPO95SW9fQD7kUCQa0+q6St28ZnnQUQAgL3DFmJCYSiR1oCuutu1NMmNqBABIlGBo/ODkgIbGplg1AgAyUIIhEYnGVFZi2sXUCAC4jBIMiUhnTDs31Wt5VXnQUQAgb1CCIXD2wkV1nhtiKBQApqEEQ6A9mpoawfxAALgSJRgCkc6Y1q5cppbVtUFHAYC8QgkWufGphA529zA1AgBmQAkWuY4T/RqZSHA8EABmQAkWuUhnTBWlJbpvW0PQUQAg71CCRS4SjenuLatUXZHrVbMAoPBRgkXsZO+ojsZHGAoFgFlQgkWsvSsmiakRADAbSrCIRTpj2tRQrc2NNUFHAYC8RAkWqbHJhJ4/2qs2hkIBYFaUYJH63rFejU8lGQoFgGugBItUe2dMVeUlunvzqqCjAEDeogSLkLsrEo1r19ZGVZWXBh0HAPIWJViEjvWM6GTfqNoYCgWAa6IEi1CkMzU1oq21KeAkAJDfKMEi1B6Nq2V1rdavqg46CgDkNUqwyIyMT+nF472cFQoAWaAEi8zB7h5NJlxt2xkKBYC5UIJFJhKNq7ayTDs3MjUCAOZCCRYRd1d7NKZ3bWtURRl/tAAwF94pi0j0/JDOXhjT3h0MhQJANijBIhLpjEsS1wsFgCxRgkUkEo3p5jXL1by8KugoAFAQKMEiceHipF5+o5+hUACYB0qwSHz3SI8SSWcVeQCYB0qwSESiMa1YVq7b1q8MOgoAFAxKsAgkk672aFy7W5tUVsofKQBki3fMInDozUH1DI9rL1eJAYB5oQSLQCQak5m0m1UjAGBeKMEiEInGdOu6lWqsrQw6CgAUFEqwwPWNTOiVUwMMhQLAdaAEC9yBrrjcxdQIALgOlGCBi0Rjaqip0NvXrgg6CgAUHEqwgCWSrue64tqzvUklJRZ0HAAoOJRgAXvl1IAGRicZCgWA60QJFrD2aEwlJu1u4aQYALgelGABi0RjunNjvVZUlwcdBQAKEiVYoGKDY/rxmUHWDgSABaAEC1R7V2oBXY4HAsD1owQLVHs0publlbppTV3QUQCgYFGCBWgykdR3unq0d/tqmTE1AgCuFyVYgF5+o19D41McDwSABaIEC1AkGlN5qWnXtoagowBAQaMEC1B7Z1zv3LRKdVVMjQCAhaAEC8yZgYuKnh/irFAAyAFKsMC0R2OSpL07uEoMACwUJVhg2qNxratfpq1NtUFHAYCCRwkWkPGphA52MzUCAHKFEiwgLx3v1+hEgqFQAMgRSrCARKIxVZSV6N4tjUFHAYCiQAkWkEg0pnu3NGhZRWnQUQCgKFCCBeKN3hEdi49o73aGQgEgVyjBAtEeTa0awaXSACB3sipBM3vAzKJm1m1mj87w+BfM7JX0V5eZDWQ89jEzO5L++lguw4dJJBrTlsYabWqsCToKABSNsrl2MLNSSY9Lul/SaUkvmdk+dz98aR93/42M/X9V0u3p26sk/Z6knZJc0svp5/bn9KcochcnEvre0V599O4NQUcBgKKSzSfBuyR1u/sxd5+Q9KSkB6+x/8OS/jZ9+32SnnH3vnTxPSPpgYUEDqMXjvVqfCrJpdIAIMeyKcG1kk5l3D+d3nYVM9soabOkZ+f7XMwuEo1pWXmp7tq8KugoAFBUcn1izEOSvuHuifk8ycweMbMOM+uIx+M5jlTY3F3Pdsa0a1uDqsqZGgEAuZRNCZ6RtD7j/rr0tpk8pLeGQrN+rrt/xd13uvvOpiamAGQ6Gh/R6f6LnBUKAIsgmxJ8SVKLmW02swqlim7f9J3MbIekeknfy9j8tKT3mlm9mdVLem96G7J0adWINuYHAkDOzXl2qLtPmdknlSqvUklPuPshM3tMUoe7XyrEhyQ96e6e8dw+M/sDpYpUkh5z977c/gjFLRKNqbW5Vuvqq4OOAgBFZ84SlCR33y9p/7Rtn552//dnee4Tkp64znyhNjw+pe8f79M/37U56CgAUJS4YkweO9jdo8mEczwQABYJJZjH2qMx1VaWaeem+qCjAEBRogTzlLsr0hnXT7Q0qryUPyYAWAy8u+apznNDOjc4xlViAGARUYJ5KpKeGrGHqREAsGgowTzV3hnXLTcuV/PyqqCjAEDRogTz0IXRSb18sp+hUABYZJRgHvpOd1yJpGvvDoZCAWAxUYJ5KNIZ18rqct22nqkRALCYKME8k0y6nuuKaXdLk0pLLOg4AFDUKME88+M3L6hneIKhUABYApRgnol0xmUm7W6hBAFgsVGCeSYSjekd61aqobYy6CgAUPQowTzSOzyuV08PMDUCAJYIJZhHDhyJy10cDwSAJUIJ5pFIZ1yNtRV6240rgo4CAKFACeaJRNL1XFdce1pXq4SpEQCwJCjBPPHKqX5duDjJUCgALCFKME9EOuMqLTH9xDZKEACWCiWYJyLRmO7cUK8V1eVBRwGA0KAE88D5wTEdenNQbQyFAsCSogTzwHPRuCQxPxAAlhglmAci0ZhuWF6lHTfUBR0FAEKFEgzYZCKp7xzp0d4dTTJjagQALCVKMGAdJ/o1PD6lNoZCAWDJUYIBa4/GVF5q2rWtMegoABA6lGDAItGY7tq8SrWVZUFHAYDQoQQDdLp/VF3nhzkrFAACQgkGqD09NYLjgQAQDEowQO3RmNavWqatTTVBRwGAUKIEAzI2mdDB7l7t3b6aqREAEBBKMCDfP96ni5MJjgcCQIAowYBEojFVlpXoni0NQUcBgNCiBAPSHo3r3q0NWlZRGnQUAAgtSjAAx3tGdLxnhKFQAAgYJRiA9mhMEqtGAEDQKMEARKJxbWmq0YaG6qCjAECoUYJLbHRiSi8c6+VTIADkAUpwiX3vaK8mppKUIADkAUpwibVH46quKNU7N9cHHQUAQo8SXELurkg0pl3bGlVZxtQIAAgaJbiEjsaHdbr/IkOhAJAnKMElFOm8tGpEU8BJAAASJbikItGYdtxQpxtXLgs6CgBAlOCSGRqb1Esn+lg7EADyCCW4RA5292oy4drLUCgA5A1KcIm0R2OqqyrTHRuZGgEA+YISXAKXpkbsbmlSeSkvOQDkC96Rl8DrZ4d0fnCcs0IBIM9Qgksgkl41Yg8lCAB5hRJcAu3RmN62drlW11UFHQUAkIESXGQXRif18hv9XCUGAPIQJbjIDhyJK+lifiAA5CFKcJFFojGtrC7XbetXBh0FADANJbiIkknXc9G49rQ2qbTEgo4DAJiGElxEPzpzQb0jExwPBIA8RQkuokg0JjNpdytTIwAgH1GCiygSjeu29Su1qqYi6CgAgBlQgoukZ3hcr50eYCgUAPJYViVoZg+YWdTMus3s0Vn2+VkzO2xmh8zsbzK2J8zslfTXvlwFz3cHuuJyFyUIAHmsbK4dzKxU0uOS7pd0WtJLZrbP3Q9n7NMi6VOSdrl7v5llvvNfdPfbcpw770WicTXWVuqWG5cHHQUAMItsPgneJanb3Y+5+4SkJyU9OG2fX5T0uLv3S5K7x3Ibs7BMJZI60BVX2/YmlTA1AgDyVjYluFbSqYz7p9PbMrVKajWzg2b2gpk9kPFYlZl1pLf/zALzFoRXTg3owsVJhkIBIM/NORw6j+/TIqlN0jpJB8zs7e4+IGmju58xsy2SnjWzH7n70cwnm9kjkh6RpA0bNuQoUnAi0ZhKS0zvamkMOgoA4Bqy+SR4RtL6jPvr0tsynZa0z90n3f24pC6lSlHufib96zFJ7ZJun/4buPtX3H2nu+9sair8OXWRzrju3FivFcvKg44CALiGbErwJUktZrbZzCokPSRp+lmef6fUp0CZWaNSw6PHzKzezCoztu+SdFhF7NyFMR0+O8hQKAAUgDmHQ919ysw+KelpSaWSnnD3Q2b2mKQOd9+Xfuy9ZnZYUkLSb7l7r5ndJ+nLZpZUqnA/n3lWaTF6rit1TtDeHYX/iRYAil1WxwTdfb+k/dO2fTrjtkv6zfRX5j7PS3r7wmMWjkhnXGtWVGl7c13QUQAAc+CKMTk0MZXUd7t71LZ9tcyYGssVTqsAAA1nSURBVAEA+Y4SzKGON/o0PD6lvdsZCgWAQkAJ5lB7NK7yUtOubUyNAIBCQAnmUKQzprs3N6imMlfTLwEAi4kSzJFTfaM6EhtWG0OhAFAwKMEcae+KS5L27mB+IAAUCkowR9o7Y9qwqlpbGmuCjgIAyBIlmANjkwkdPNqjvdubmBoBAAWEEsyBF4/3aWwyqTaGQgGgoFCCORDpjKmyrET3bmkIOgoAYB4owRxoj8Z039YGVZWXBh0FADAPlOACHe8Z0YneUc4KBYACRAkuUKQztWpEWyslCACFhhJcoEg0pq1NNdrQUB10FADAPFGCCzA6MaUXj/WxgC4AFChKcAGe7+7VRCLJ8UAAKFCU4AJEojHVVJRq56b6oKMAAK4DJXid3F3t0bh2bWtUZRlTIwCgEFGC1+lIbFhnBi4yFAoABYwSvE6Xp0awdBIAFCxK8DpFojHtuKFOa1YsCzoKAOA6UYLXYXBsUh0n+hkKBYACRwleh4NHejSVdOYHAkCBowSvQyQaU11Vme7YsDLoKACABaAE5+nS1IjdrU0qK+XlA4BCxrv4PB0+O6jY0DhDoQBQBCjBeWqPxiVJe1qZGgEAhY4SnKdIZ0y3rluhprrKoKMAABaIEpyHgdEJ/eBkv9oYCgWAokAJzsOBIz1KurSXq8QAQFGgBOehvTOmVTUVunUdUyMAoBhQgllKJl3tXXHtaW1SaYkFHQcAkAOUYJZeO3NBfSMTXDAbAIoIJZilSGdMJSbtbqEEAaBYUIJZao/GdPuGetXXVAQdBQCQI5RgFuJD43r19AXOCgWAIkMJZuFAV+oqMcwPBIDiQglmIRKNaXVdpW65cXnQUQAAOUQJzmEqkdSBrrjatjfJjKkRAFBMKME5/PDUgAbHplg1AgCKECU4h0hnTGUlpl0tjUFHAQDkGCU4h0g0rjs31mt5VXnQUQAAOUYJXsO5C2N6/eyg9u5gKBQAihEleA3t0ZgkcTwQAIoUJXgNkWhMN66oUmtzbdBRAACLgBKcxcRUUt890qO2HauZGgEARYoSnEXHiT6NTCQYCgWAIkYJziISjamitET3bW0IOgoAYJFQgrOIROO6e8sq1VSWBR0FALBIKMEZnOobVXdsmAtmA0CRowRn8NbUCJZOAoBiRgnOIBKNa2NDtTY31gQdBQCwiCjBacYmE3r+aI/2bmdqBAAUO0pwmheO9WpsMqk2hkIBoOhRgtO0R+OqKi/RPVuYGgEAxY4SzODuerYzpvu2NqqqvDToOACARUYJZjjeM6KTfaOcFQoAIUEJZohE45LE/EAACAlKMEN7NKZtq2u1flV10FEAAEsgqxI0swfMLGpm3Wb26Cz7/KyZHTazQ2b2NxnbP2ZmR9JfH8tV8FwbGZ/Si8f6GAoFgBCZ88KYZlYq6XFJ90s6LeklM9vn7ocz9mmR9ClJu9y938xWp7evkvR7knZKckkvp5/bn/sfZWGeP9qriUSSVSMAIESy+SR4l6Rudz/m7hOSnpT04LR9flHS45fKzd1j6e3vk/SMu/elH3tG0gO5iZ5bkWhMNRWl2rlpVdBRAABLJJsSXCvpVMb90+ltmVoltZrZQTN7wcwemMdzZWaPmFmHmXXE4/Hs0+eIu6u9M6Z3tTSqoozDpAAQFrl6xy+T1CKpTdLDkv7MzFZm+2R3/4q773T3nU1NS39Mruv8sN68MMZQKACETDYleEbS+oz769LbMp2WtM/dJ939uKQupUoxm+cGLpJeNYKpEQAQLtmU4EuSWsxss5lVSHpI0r5p+/ydUp8CZWaNSg2PHpP0tKT3mlm9mdVLem96W16JdMZ005rlumFFVdBRAABLaM4SdPcpSZ9Uqrxel/R1dz9kZo+Z2YfSuz0tqdfMDkuKSPotd+919z5Jf6BUkb4k6bH0trwxODapjjf6mRoBACE05xQJSXL3/ZL2T9v26YzbLuk301/Tn/uEpCcWFnPxfPdIjxJJ194dDIUCQNiE/lTISGdMy6vKdPv6rM/jAQAUiVCXYDLpau+Ka3drk8pKQ/1SAEAohfqd//DZQcWHxpkaAQAhFeoSjHSmpkbs4aQYAAilcJdgNKZ3rFuhxtrKoKMAAAIQ2hLsG5nQD08NMEEeAEIstCX4nSNxuYupEQAQYqEtwUhnTA01Fbp17YqgowAAAhLKEkwkXc91xbWntUklJRZ0HABAQEJZgq+eHlD/6KTaGAoFgFALZQm2d8ZUYtLulsagowAAAhTKEoxE47pjQ71WVlcEHQUAEKDQlWB8aFw/OnOBs0IBAOErwee64pKkNq4SAwChF7oSjERjWl1XqZvXLA86CgAgYKEqwalEUge64tq7fbXMmBoBAGEXqhL8wckBDY1Nae8OhkIBACErwUg0prIS065tTI0AAIStBDtjeuemVaqrKg86CgAgD4SmBM9euKjOc0MMhQIALgtNCbZHU1MjWEUeAHBJaEow0hnT2pXLtG11bdBRAAB5IhQlOD6V0MHuHu3d0cTUCADAZaEowY4T/RqZSDAUCgC4QihKMNIZU0VZie7d2hB0FABAHglHCUZjumdLg6oryoKOAgDII0Vfgid7R3U0PqK9XDAbADBN0Zdge1dMElMjAABXK/oSjHTGtLmxRpsaa4KOAgDIM0VdgmOTCT1/tJe1AwEAMyrqEvzesV6NTyXVxlAoAGAGRV2C7Z0xVZWX6O7Nq4KOAgDIQ0Vbgu6uSDSuXVsbVVVeGnQcAEAeKtoSPNYzopN9o2rbwVAoAGBmRVuCkc7U1Ii2Vk6KAQDMrGhLsD0aV8vqWq1fVR10FABAnirK64i5u+qqynTHxjVBRwEA5LGiLEEz05/+3J1BxwAA5LmiHQ4FAGAulCAAILQoQQBAaFGCAIDQogQBAKFFCQIAQosSBACEFiUIAAgtShAAEFqUIAAgtChBAEBoUYIAgNCiBAEAoUUJAgBCixIEAIQWJQgACC1KEAAQWpQgACC0zN2DznAFM4tLeiNH365RUk+OvtdSIvfSIvfSKtTcUuFmD3vuje7eNNMDeVeCuWRmHe6+M+gc80XupUXupVWouaXCzU7u2TEcCgAILUoQABBaxV6CXwk6wHUi99Ii99Iq1NxS4WYn9yyK+pggAADXUuyfBAEAmFVRlKCZPWBmUTPrNrNHZ3i80sz+Z/rxF81s09KnvFoWuT9uZnEzeyX99S+CyDkt0xNmFjOzH8/yuJnZH6d/ptfM7I6lzjiTLHK3mdmFjNf600udcSZmtt7MImZ22MwOmdmvzbBP3r3mWebOu9fczKrM7Ptm9mo692dm2Cdf30+yyZ537ymSZGalZvZDM/vmDI8t7uvt7gX9JalU0lFJWyRVSHpV0s3T9vlXkr6Uvv2QpP9ZILk/LulPgs46LdNuSXdI+vEsj/+UpG9LMkn3SHox6MxZ5m6T9M2gc86Qa42kO9K36yR1zfD3JO9e8yxz591rnn4Na9O3yyW9KOmeafvk3fvJPLLn3XtKOtdvSvqbmf4+LPbrXQyfBO+S1O3ux9x9QtKTkh6cts+Dkr6avv0NSe82M1vCjDPJJnfecfcDkvquscuDkv6Hp7wgaaWZrVmadLPLIndecvez7v6D9O0hSa9LWjttt7x7zbPMnXfSr+Fw+m55+mv6iRP5+H6Sbfa8Y2brJP20pD+fZZdFfb2LoQTXSjqVcf+0rv7Hdnkfd5+SdEFSw5Kkm102uSXpw+khrm+Y2fqlibYg2f5c+eje9FDSt83slqDDTJceBrpdqf/hZ8rr1/wauaU8fM3TQ3OvSIpJesbdZ3298+j9RFJW2aX8e0/5L5L+naTkLI8v6utdDCVYzJ6StMndb5X0jN763xBy7wdKXVrpHZK+KOnvAs5zBTOrlfS/Jf26uw8GnSdbc+TOy9fc3RPufpukdZLuMrO3BZ0pW1lkz6v3FDP7gKSYu78cVIZiKMEzkjL/N7MuvW3GfcysTNIKSb1Lkm52c+Z29153H0/f/XNJdy5RtoXI5s8j77j74KWhJHffL6nczBoDjiVJMrNypYrkr939/8ywS16+5nPlzufXXJLcfUBSRNID0x7Kx/eTK8yWPQ/fU3ZJ+pCZnVDqkNBPmtnXpu2zqK93MZTgS5JazGyzmVUodeB037R99kn6WPr2RyQ96+mjrAGaM/e04zofUuq4Sr7bJ+kX0mcs3iPpgrufDTrUXMzshkvHGczsLqX+bQT+xpbO9N8lve7u/3mW3fLuNc8mdz6+5mbWZGYr07eXSbpfUue03fLx/SSr7Pn2nuLun3L3de6+San3wGfd/eem7baor3dZrr5RUNx9ysw+Kelppc64fMLdD5nZY5I63H2fUv8Y/8rMupU6OeKh4BKnZJn7X5vZhyRNKZX744EFTjOzv1XqrL5GMzst6feUOgAvd/+SpP1Kna3YLWlU0j8LJumVssj9EUn/0symJF2U9FA+vLEp9T/ln5f0o/SxHkn695I2SHn9mmeTOx9f8zWSvmpmpUqV8tfd/Zv5/n6Slk32vHtPmclSvt5cMQYAEFrFMBwKAMB1oQQBAKFFCQIAQosSBACEFiUIAAgtShAAEFqUIAAgtChBAEBo/X8DflpFfcN4UgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwSE_386uhxD"
      },
      "source": [
        "Run a prediction on a new sentence:\n",
        "\n",
        "If the prediction is >= 0.0, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXgfQSgRW6zU"
      },
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H80VjrOCk3GX"
      },
      "source": [
        "We can further improve the inference on new sentences by improving the way we present the predicted results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6-giIqikvJ9"
      },
      "source": [
        "# prediction is >= 0.0 --> positive review\n",
        "# else --> negative review\n",
        "\n",
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "    \"Skip this movie.\",\n",
        "    \"Don't waste your time.\",\n",
        "]\n",
        "\n",
        "predicted_scores = model.predict(np.array(inputs))\n",
        "predicted_labels = [\"Positive\" if x>=0.0 else \"Negative\" for x in predicted_scores]\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g1evcaRpTKm"
      },
      "source": [
        "## Step 3 -> Assignment: Additional Architectures - Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "![layered_bidirectional](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/layered_bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSClCrG1z8l"
      },
      "source": [
        "The interesting thing about using an `RNN` with `return_sequences=True` is that the output still has 3-axes, like the input, so it can be passed to another RNN layer, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo1jjO3vn0jo"
      },
      "source": [
        "#step 3\n",
        "model_2 = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "\n",
        "    # Additional layer goes there.\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(..., return_sequences=False), # fill the number of LSTM units in place of ...\n",
        "\n",
        "    # if you want to add another layer to this network, remember to set return_sequences=True\n",
        "    # in the previous layer. We want the a multi-layer LSTM network to return the sequences to the\n",
        "    # following LSTM layer.\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEPV5jVGp-is"
      },
      "source": [
        "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeSE-YjdqAeN"
      },
      "source": [
        "#step 4\n",
        "history_2 = model_2.fit(train_dataset, epochs=5,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LdwilM1qPM3"
      },
      "source": [
        "#step 5\n",
        "test_loss_2, test_acc_2 = model_2.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss_2)\n",
        "print('Test Accuracy:', test_acc_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykUKnAoqbycW"
      },
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was not good. The animation and the graphics '\n",
        "               'were terrible. I would not recommend this movie.')\n",
        "predictions_2 = model_2.predict(np.array([sample_text]))\n",
        "print(predictions_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YYub0EDtwCu"
      },
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history_2, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history_2, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeX7udCOgITx"
      },
      "source": [
        "# 0 --> negative review\n",
        "# 1 --> positive review\n",
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "    \"Skip this movie.\",\n",
        "    \"Don't waste your time.\",\n",
        "]\n",
        "\n",
        "predicted_scores_2 = model_2.predict(inputs)\n",
        "predicted_labels_2 = [\"Positive\" if x>=0.0 else \"Negative\" for x in predicted_scores_2]\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels_2):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}